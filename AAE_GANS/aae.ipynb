{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aae_new.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTZchEsNUoQQ",
        "colab_type": "text"
      },
      "source": [
        "# Adversarial Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwjUqkETUrmT",
        "colab_type": "text"
      },
      "source": [
        "## Project Info\n",
        "this notebook is an implementation Adversarial Autoencoders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diqWdXg7Uw-v",
        "colab_type": "text"
      },
      "source": [
        "# Header\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8UlGWIXUzqr",
        "colab_type": "text"
      },
      "source": [
        " ## Import Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpQplgH1UwVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Basic Level Libs\n",
        "import math\n",
        "import os \n",
        "import itertools\n",
        "\n",
        "#Mid Level Libs\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "#FrameWorks\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "#Other\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd0qH27vU57O",
        "colab_type": "text"
      },
      "source": [
        "## Mout Google Dirve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCp0XuYBUng_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK938EZGacyH",
        "colab_type": "text"
      },
      "source": [
        "##config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAMvGqnWYEgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "seed = 10\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "n_classes = 10\n",
        "z_dim = 2\n",
        "X_dim = 784\n",
        "y_dim = 10\n",
        "train_batch_size = 100\n",
        "valid_batch_size = 100\n",
        "N = 1000\n",
        "epochs = 500\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bISeEkeNVAxg",
        "colab_type": "text"
      },
      "source": [
        "# Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5NuXL5fRaC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class subMNIST(MNIST):\n",
        "\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None,\n",
        "                 download=False, k=3000):\n",
        "        super(subMNIST, self).__init__(root, train, transform,\n",
        "                                       target_transform, download)\n",
        "        self.k = k\n",
        "        self.test_dataa = None\n",
        "        self.test_labelsa = None\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return self.k\n",
        "        else:\n",
        "            return 10000\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                                     (0.5, 0.5, 0.5))])\n",
        "trainset = subMNIST(root='../data', train=True,\n",
        "                    download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "print(len(trainset))\n",
        "print(len(trainloader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERiwRMCURadP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "trainset_original = datasets.MNIST('/content/', train=True, download=True,\n",
        "                                   transform=transform)\n",
        "\n",
        "train_label_index = []\n",
        "valid_label_index = []\n",
        "for i in range(10):\n",
        "    train_label_list = trainset_original.train_labels.numpy()\n",
        "    label_index = np.where(train_label_list == i)[0]\n",
        "    label_subindex = list(label_index[:300])\n",
        "    valid_subindex = list(label_index[300: 1000 + 300])\n",
        "    train_label_index += label_subindex\n",
        "    valid_label_index += valid_subindex\n",
        "\n",
        "trainset_np = trainset_original.train_data.numpy()\n",
        "trainset_label_np = trainset_original.train_labels.numpy()\n",
        "train_data_sub = torch.from_numpy(trainset_np[train_label_index])\n",
        "train_labels_sub = torch.from_numpy(trainset_label_np[train_label_index])\n",
        "\n",
        "trainset_new = subMNIST(root='/content/', train=True, download=True, transform=transform, k=3000)\n",
        "trainset_new.train_dataa = None#train_data_sub.clone()\n",
        "trainset_new.train_labelsa = train_labels_sub.clone()\n",
        "\n",
        "pickle.dump(trainset_new, open(\"/content/train_labeled.p\", \"wb\"))\n",
        "\n",
        "validset_np = trainset_original.train_data.numpy()\n",
        "validset_label_np = trainset_original.train_labels.numpy()\n",
        "valid_data_sub = torch.from_numpy(validset_np[valid_label_index])\n",
        "valid_labels_sub = torch.from_numpy(validset_label_np[valid_label_index])\n",
        "\n",
        "validset = subMNIST(root='/content/data', train=False, download=True, transform=transform, k=10000)\n",
        "validset.test_dataa = valid_data_sub.clone()\n",
        "validset.test_labelsa = valid_labels_sub.clone()\n",
        "\n",
        "pickle.dump(validset, open(\"/content/validation.p\", \"wb\"))\n",
        "\n",
        "train_unlabel_index = []\n",
        "for i in range(60000):\n",
        "    if i in train_label_index or i in valid_label_index:\n",
        "        pass\n",
        "    else:\n",
        "        train_unlabel_index.append(i)\n",
        "\n",
        "trainset_np = trainset_original.train_data.numpy()\n",
        "trainset_label_np = trainset_original.train_labels.numpy()\n",
        "train_data_sub_unl = torch.from_numpy(trainset_np[train_unlabel_index])\n",
        "train_labels_sub_unl = torch.from_numpy(trainset_label_np[train_unlabel_index])\n",
        "\n",
        "trainset_new_unl = subMNIST(root='/content/data', train=True, download=True, transform=transform, k=47000)\n",
        "trainset_new_unl.train_dataa = train_data_sub_unl.clone()\n",
        "trainset_new_unl.train_labelsa = None      # Unlabeled\n",
        "\n",
        "trainset_new_unl.train_labelsa\n",
        "\n",
        "pickle.dump(trainset_new_unl, open(\"/content/train_unlabeled.p\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NusOTDhlVjWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_path='/content/'):\n",
        "    print('loading data!')\n",
        "    trainset_labeled = pickle.load(open(data_path + \"train_labeled.p\", \"rb\"))\n",
        "    trainset_unlabeled = pickle.load(open(data_path + \"train_unlabeled.p\", \"rb\"))\n",
        "    # Set -1 as labels for unlabeled data\n",
        "    trainset_unlabeled.train_labelsaaa = torch.from_numpy(np.array([-1] * 47000))\n",
        "    validset = pickle.load(open(data_path + \"validation.p\", \"rb\"))\n",
        "\n",
        "    train_labeled_loader = torch.utils.data.DataLoader(trainset_labeled,\n",
        "                                                       batch_size=train_batch_size,\n",
        "                                                       shuffle=True, **kwargs)\n",
        "\n",
        "    train_unlabeled_loader = torch.utils.data.DataLoader(trainset_unlabeled,\n",
        "                                                         batch_size=train_batch_size,\n",
        "                                                         shuffle=True, **kwargs)\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(validset, batch_size=valid_batch_size, shuffle=True)\n",
        "\n",
        "    return train_labeled_loader, train_unlabeled_loader, valid_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JINlTjUBVk7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labeled_loader, train_unlabeled_loader, valid_loader = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gUWf7n3WuWm",
        "colab_type": "text"
      },
      "source": [
        "##Displayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0F9a6S1VWsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img):\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)).squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXykvt1tVYps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def displaying_data(dataiter):\n",
        "    # obtain one batch of training images\n",
        "    images, labels = dataiter.next()\n",
        "    images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "    # plot the images in the batch, along with the corresponding labels\n",
        "    fig = plt.figure(figsize=(25, 4))\n",
        "    # display 20 images\n",
        "    for idx in np.arange(20):\n",
        "        ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "        try:\n",
        "            imshow(images[idx][0])\n",
        "        except:\n",
        "            imshow(images[idx])\n",
        "\n",
        "        ax.set_title(str(int(labels[idx])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzNfsHVvVaUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterdata = iter(train_labeled_loader)\n",
        "displaying_data(iterdata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ECkatzhW1Gr",
        "colab_type": "text"
      },
      "source": [
        "## Image saver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3EkTeosW4rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = '/content'\n",
        "#!rm -rf '/content/drive/My Drive/images/'\n",
        "if not os.path.exists(os.path.join(base_path, 'images')):\n",
        "    os.mkdir(os.path.join(base_path, 'images'))\n",
        "\n",
        "def sample_image(encoder, epoch):\n",
        "    \"\"\"Saves a grid of generated digits\"\"\"\n",
        "    # Sample noise\n",
        "    z = np.array([np.array([num,num2]) for num2 in range(-10,10) for num in range(-10,10)])\n",
        "    z = Variable(Tensor(np.array(z)))\n",
        "\n",
        "    gen_imgs = encoder(z.cuda())\n",
        "\n",
        "    gen_imgs = gen_imgs.view(-1,1,28,28)\n",
        "\n",
        "    save_image(gen_imgs.data, os.path.join(base_path, 'images/')+\"%d.png\" % epoch, nrow=20, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck8baaD0XTw3",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF43hpboXV9L",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdKdvwozXTe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lin1 = nn.Linear(X_dim, N)\n",
        "        self.lin2 = nn.Linear(N, N)\n",
        "        # Gaussian code (z)\n",
        "        self.lin3gauss = nn.Linear(N, z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
        "        x = F.relu(x)\n",
        "        xgauss = self.lin3gauss(x)\n",
        "\n",
        "        return xgauss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-C92W83Xiyv",
        "colab_type": "text"
      },
      "source": [
        "##Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv51nC-0XiSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lin1 = nn.Linear(z_dim, N)\n",
        "        self.lin2 = nn.Linear(N, N)\n",
        "        self.lin3 = nn.Linear(N, X_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = F.relu(x)\n",
        "        x = self.lin2(x)\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.lin3(x)\n",
        "        return F.sigmoid(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW7mBqsdXqLz",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp91mgPgXpEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.lin1 = nn.Linear(z_dim, N)\n",
        "        self.lin2 = nn.Linear(N, N)\n",
        "        self.lin3 = nn.Linear(N, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return F.sigmoid(self.lin3(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77wkrq66Xzux",
        "colab_type": "text"
      },
      "source": [
        "# Workers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGksGjJ1YMdI",
        "colab_type": "text"
      },
      "source": [
        "##Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCunCVi-YJiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, filename):\n",
        "    print('Best model so far, saving it...')\n",
        "    torch.save(model.state_dict(), filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_78eGRNYYOyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report_loss(epoch, D_loss_gauss, G_loss, recon_loss):\n",
        "    '''\n",
        "    Print loss\n",
        "    '''\n",
        "    print('Epoch-{}; D_loss_gauss: {:.4}; G_loss: {:.4}; recon_loss: {:.4}'.format(epoch, \n",
        "                                                                                   D_loss_gauss.data,\n",
        "                                                                                   G_loss.data,\n",
        "                                                                                   recon_loss.data))                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx_a0LyPamdG",
        "colab_type": "text"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9aO_XKYTw3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir \"/content/graph\"\n",
        "def create_latent(Q, loader, e):\n",
        "    '''\n",
        "    Creates the latent representation for the samples in loader\n",
        "    return:\n",
        "        z_values: numpy array with the latent representations\n",
        "        labels: the labels corresponding to the latent representations\n",
        "    '''\n",
        "    Q.eval()\n",
        "    labels = []\n",
        "\n",
        "    for batch_idx, (X, target) in enumerate(loader):\n",
        "\n",
        "        X = X * 0.3081 + 0.1307\n",
        "        X.resize_(train_batch_size, X_dim)\n",
        "\n",
        "        # X.resize_(loader.batch_size, X_dim)\n",
        "        X, target = Variable(X), Variable(target)\n",
        "        labels.extend(target.data.tolist())\n",
        "        if cuda:\n",
        "            X, target = X.cuda(), target.cuda()\n",
        "        # Reconstruction phase\n",
        "        z_sample = Q(X)\n",
        "        if batch_idx > 0:\n",
        "            z_values = np.concatenate((z_values, np.array(z_sample.data.tolist())))\n",
        "        else:\n",
        "            z_values = np.array(z_sample.data.tolist())\n",
        "    labels = np.array(labels)\n",
        "    import matplotlib.pyplot as plt\n",
        "    %matplotlib inline\n",
        "\n",
        "    plt.scatter(z_values[:,0], z_values[:, 1], c=labels)\n",
        "    plt.savefig('/content/graph/{}.png'.format(e))\n",
        "    \n",
        "\n",
        "    return z_values, labels "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydjo8oKVRfNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(P, Q, D_gauss, P_decoder, Q_encoder, Q_generator, D_gauss_solver, data_loader):\n",
        "    '''\n",
        "    Train procedure for one epoch.\n",
        "    '''\n",
        "    TINY = 1e-15\n",
        "    # Set the networks in train mode (apply dropout when needed)\n",
        "    Q.train()\n",
        "    P.train()\n",
        "    D_gauss.train()\n",
        "\n",
        "    # Loop through the labeled and unlabeled dataset getting one batch of samples from each\n",
        "    # The batch size has to be a divisor of the size of the dataset or it will return\n",
        "    # invalid samples\n",
        "    for X, target in data_loader:\n",
        "\n",
        "        # Load batch and normalize samples to be between 0 and 1\n",
        "        X = X * 0.3081 + 0.1307\n",
        "        X.resize_(train_batch_size, X_dim)\n",
        "        X, target = Variable(X), Variable(target)\n",
        "        if cuda:\n",
        "            X, target = X.cuda(), target.cuda()\n",
        "\n",
        "        # Init gradients\n",
        "        P.zero_grad()\n",
        "        Q.zero_grad()\n",
        "        D_gauss.zero_grad()\n",
        "\n",
        "        #######################\n",
        "        # Reconstruction phase\n",
        "        #######################\n",
        "        z_sample = Q(X)\n",
        "        X_sample = P(z_sample)\n",
        "        recon_loss = F.binary_cross_entropy(X_sample + TINY, X.resize(train_batch_size, X_dim) + TINY)\n",
        "\n",
        "        recon_loss.backward()\n",
        "        P_decoder.step()\n",
        "        Q_encoder.step()\n",
        "\n",
        "        P.zero_grad()\n",
        "        Q.zero_grad()\n",
        "        D_gauss.zero_grad()\n",
        "\n",
        "        #######################\n",
        "        # Regularization phase\n",
        "        #######################\n",
        "        # Discriminator\n",
        "        Q.eval()\n",
        "        z_real_gauss = Variable(Tensor(np.random.randn(train_batch_size, z_dim) * 5.))#Variable(n.sample((train_batch_size, z_dim))).squeeze()\n",
        "        if cuda:\n",
        "            z_real_gauss = z_real_gauss.cuda()\n",
        "\n",
        "        z_fake_gauss = Q(X)\n",
        "        D_real_gauss = D_gauss(z_real_gauss)\n",
        "        D_fake_gauss = D_gauss(z_fake_gauss)\n",
        "\n",
        "        D_loss = -torch.mean(torch.log(D_real_gauss + TINY) + torch.log(1 - D_fake_gauss + TINY))\n",
        "\n",
        "        D_loss.backward()\n",
        "        D_gauss_solver.step()\n",
        "\n",
        "        P.zero_grad()\n",
        "        Q.zero_grad()\n",
        "        D_gauss.zero_grad()\n",
        "\n",
        "        # Generator\n",
        "        Q.train()\n",
        "        z_fake_gauss = Q(X)\n",
        "\n",
        "        D_fake_gauss = D_gauss(z_fake_gauss)\n",
        "        G_loss = -torch.mean(torch.log(D_fake_gauss + TINY))\n",
        "\n",
        "        G_loss.backward()\n",
        "        Q_generator.step()\n",
        "\n",
        "        P.zero_grad()\n",
        "        Q.zero_grad()\n",
        "        D_gauss.zero_grad()\n",
        "\n",
        "    return D_loss, G_loss, recon_loss\n",
        "\n",
        "\n",
        "def generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader):\n",
        "    best_loss = np.Inf\n",
        "    torch.manual_seed(10)\n",
        "    global P\n",
        "    if cuda:\n",
        "        Q = Encoder().cuda()\n",
        "        P = Decoder().cuda()\n",
        "        D_gauss = Discriminator().cuda()\n",
        "    else:\n",
        "        Q = Q_net()\n",
        "        P = P_net()\n",
        "        D_gauss = D_net_gauss()\n",
        "\n",
        "    # Set learning rates\n",
        "    gen_lr = 0.0001\n",
        "    reg_lr = 0.00005\n",
        "\n",
        "    # Set optimizators\n",
        "    P_decoder = optim.Adam(P.parameters(), lr=gen_lr)\n",
        "    Q_encoder = optim.Adam(Q.parameters(), lr=gen_lr)\n",
        "\n",
        "    Q_generator = optim.Adam(Q.parameters(), lr=reg_lr)\n",
        "    D_gauss_solver = optim.Adam(D_gauss.parameters(), lr=reg_lr)\n",
        "\n",
        "    for epoch in range(700):\n",
        "        sample_image(P,epoch)\n",
        "        create_latent(Q, valid_loader,epoch) \n",
        "\n",
        "        D_loss_gauss, G_loss, recon_loss = train(P, Q, D_gauss, P_decoder, Q_encoder,\n",
        "                                                 Q_generator,\n",
        "                                                 D_gauss_solver,\n",
        "                                                 train_unlabeled_loader)\n",
        "        report_loss(epoch, D_loss_gauss, G_loss, recon_loss)\n",
        "        if best_loss > G_loss:\n",
        "            torch.save(P.state_dict(),'/content/P.pth')\n",
        "            torch.save(Q.state_dict(),'/content/Q.pth')\n",
        "            print(\"\\rsaved\",end='')\n",
        "            best_loss = G_loss\n",
        "    return Q, P\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_labeled_loader, train_unlabeled_loader, valid_loader = load_data()    \n",
        "    Q, P = generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9OBy0hBu_iz",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGo6n2VSu-uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter = iter(train_labeled_loader)\n",
        "data = dataiter.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKXOdFV9vOe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('orginal')\n",
        "image = data[0][0]\n",
        "plt.imshow(image[0])\n",
        "plt.show()\n",
        "save_image(image,'orginal.png')\n",
        "\n",
        "print('image after encoding and decoding')\n",
        "Q = Encoder().cuda()\n",
        "Q.load_state_dict(torch.load(\"/content/Q.pth\"))\n",
        "P = Decoder().cuda()\n",
        "P.load_state_dict(torch.load(\"/content/P.pth\"))\n",
        "encode = Q(transforms.ToTensor()(transforms.ToPILImage()(image)).view(-1,784 ).cuda())\n",
        "plt.imshow(decode.cpu().view(28,28).detach().numpy())\n",
        "plt.show()\n",
        "save_image(decode.cpu().view(28,28),'encodeddecoded.png')\n",
        "\n",
        "print('generated from distribution')\n",
        "decode = P(torch.FloatTensor([0,-10]).cuda())\n",
        "plt.imshow(decode.cpu().view(28,28).detach().numpy())\n",
        "save_image(decode.cpu().view(28,28).detach(), 'dist.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrj7kkyKjfJB",
        "colab_type": "text"
      },
      "source": [
        "#GIF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtEzGojxje3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwXM8rYGjjCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with imageio.get_writer('AAE_GAN.gif', mode='I') as writer:\n",
        "  \n",
        "  files = os.listdir('/content/images')\n",
        "  new_files = [int(f[:-4]) for f in files]\n",
        "  new_files.sort()\n",
        "  new_files = new_files[::3]\n",
        "  filenames = ['/content/images/' + str(f)+'.png' for f in new_files]\n",
        "\n",
        "  last = -1\n",
        "  for i,filename in enumerate(filenames):\n",
        "    frame = 2*(i**0.5)\n",
        "    if round(frame) > round(last):\n",
        "      last = frame\n",
        "    else:\n",
        "      continue\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)\n",
        "    \n",
        "# this is a hack to display the gif inside the notebook\n",
        "os.system('cp aaegan.gif AAE_GAN.gif.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1t5VsefnPhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with imageio.get_writer('AAE_GAN_graph.gif', mode='I') as writer:\n",
        "  \n",
        "  files = os.listdir('/content/graph')\n",
        "  new_files = [int(f[:-4]) for f in files]\n",
        "  new_files.sort()\n",
        "  new_files = new_files[::3]\n",
        "  filenames = ['/content/graph/' + str(f)+'.png' for f in new_files]\n",
        "\n",
        "  last = -1\n",
        "  for i,filename in enumerate(filenames):\n",
        "    frame = 2*(i**0.5)\n",
        "    if round(frame) > round(last):\n",
        "      last = frame\n",
        "    else:\n",
        "      continue\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)\n",
        "    \n",
        "# this is a hack to display the gif inside the notebook\n",
        "os.system('cp aaegan.gif AAE_GAN_graph.gif.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}